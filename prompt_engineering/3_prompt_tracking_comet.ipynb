{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iruud8QA-hma"
      },
      "source": [
        "# Logging, Tracking, and Debugging Prompts using Comet\n",
        "\n",
        "In this section, we will demonstrate how to log, track, and debug prompt using the `comet-llm` library. `comet-llm` is an open-sourced repo managed by Comet. Please give the repo star if you have a chance and submit any feedback you have! https://github.com/comet-ml/comet-llm\n",
        "\n",
        "Let's first load all the necessary libraries:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cLNconIi-rhZ"
      },
      "outputs": [],
      "source": [
        "! pip install openai opik --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "WcRfRjy6-hmc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "OPIK: Configuration saved to file: /home/micha/.opik.config\n"
          ]
        }
      ],
      "source": [
        "from openai import OpenAI\n",
        "import opik\n",
        "import os\n",
        "import IPython\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import urllib\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "#load the environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# API configuration\n",
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "COMET_API_KEY = os.getenv(\"COMET_API_KEY\")\n",
        "COMET_WORKSPACE = os.getenv(\"COMMET_WORKSPACE\")\n",
        "\n",
        "client = OpenAI(api_key= OPENAI_API_KEY)\n",
        "\n",
        "# Configure opik\n",
        "opik.configure()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8yyTvrK-hmd"
      },
      "source": [
        "The function below helps to generate the final results from the model after calling the OpenAI API:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EiKIWPuQ-hmd"
      },
      "outputs": [],
      "source": [
        "# completion function\n",
        "def get_completion(messages, model=\"gpt-40\", temperature=0, max_tokens=300):\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=temperature,\n",
        "        max_tokens=max_tokens\n",
        "    )\n",
        "    return response.choices[0].message[\"content\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHLJxr5B-hmd"
      },
      "source": [
        "### Load the Data\n",
        "\n",
        "The code below loads both the few-shot demonstrations and the validation dataset used for testing the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "h1Q29ooc-hme"
      },
      "outputs": [],
      "source": [
        "\n",
        "# print markdown\n",
        "def print_markdown(text):\n",
        "    \"\"\"Prints text as markdown\"\"\"\n",
        "    IPython.display.display(IPython.display.Markdown(text))\n",
        "\n",
        "# load validation data from GitHub\n",
        "f = urllib.request.urlopen(\"https://raw.githubusercontent.com/comet-ml/comet-llmops/main/data/article-tags.json\")\n",
        "val_data = json.load(f)\n",
        "\n",
        "# load few shot data from GitHub\n",
        "f = urllib.request.urlopen(\"https://raw.githubusercontent.com/comet-ml/comet-llmops/main/data/few_shot.json\")\n",
        "few_shot_data = json.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkjHAZW0-hme"
      },
      "source": [
        "The following is a helper function to obtain the final predictions from the model given a prompt template (e.g., zero-shot or few-shot) and the provided input data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FOPd0RiP-hme"
      },
      "outputs": [],
      "source": [
        "def get_predictions(prompt_template, inputs):\n",
        "\n",
        "    responses = []\n",
        "\n",
        "    for i in range(len(inputs)):\n",
        "        messages = messages = [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": prompt_template.format(input=inputs[i])\n",
        "            }\n",
        "        ]\n",
        "        response = get_completion(messages)\n",
        "        responses.append(response)\n",
        "\n",
        "    return responses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Sbh8Rl1-hme"
      },
      "source": [
        "### Few-Shot\n",
        "\n",
        "First, we define a few-shot template which will leverage the few-shot demonstration data loaded previously."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Ur-R41tr-hme"
      },
      "outputs": [],
      "source": [
        "# function to define the few-shot template\n",
        "def get_few_shot_template(few_shot_prefix, few_shot_suffix, few_shot_examples):\n",
        "    return few_shot_prefix + \"\\n\\n\" + \"\\n\".join([ \"Abstract: \"+ ex[\"abstract\"] + \"\\n\" + \"Tags: \" + str(ex[\"tags\"]) + \"\\n\" for ex in few_shot_examples]) + \"\\n\\n\" + few_shot_suffix\n",
        "\n",
        "# function to sample few shot data\n",
        "def random_sample_data (data, n):\n",
        "    return np.random.choice(few_shot_data, n, replace=False)\n",
        "\n",
        "\n",
        "# the few-shot prefix and suffix\n",
        "few_shot_prefix = \"\"\"Your task is to extract model names from machine learning paper abstracts. Your response is an an array of the model names in the format [\\\"model_name\\\"]. If you don't find model names in the abstract or you are not sure, return [\\\"NA\\\"]\"\"\"\n",
        "few_shot_suffix = \"\"\"Abstract: {input}\\nTags:\"\"\"\n",
        "\n",
        "# load 3 samples from few shot data\n",
        "few_shot_template = get_few_shot_template(few_shot_prefix, few_shot_suffix, random_sample_data(few_shot_data, 3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "z716KoK2-hmf",
        "outputId": "8552777f-809c-4c1c-e880-3071ee584a5f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Your task is to extract model names from machine learning paper abstracts. Your response is an an array of the model names in the format [\"model_name\"]. If you don\\'t find model names in the abstract or you are not sure, return [\"NA\"]\\n\\nAbstract: Children\\'s drawings have a wonderful inventiveness, creativity, and variety to them. We present a system that automatically animates children\\'s drawings of the human figure, is robust to the variance inherent in these depictions, and is simple and straightforward enough for anyone to use. We demonstrate the value and broad appeal of our approach by building and releasing the Animated Drawings Demo, a freely available public website that has been used by millions of people around the world. We present a set of experiments exploring the amount of training data needed for fine-tuning, as well as a perceptual study demonstrating the appeal of a novel twisted perspective retargeting technique. Finally, we introduce the Amateur Drawings Dataset, a first-of-its-kind annotated dataset, collected via the public demo, containing over 178,000 amateur drawings and corresponding user-accepted character bounding boxes, segmentation masks, and joint location annotations.\\nTags: [\\'NA\\']\\n\\nAbstract: Large Language Models (LLMs), such as ChatGPT and GPT-4, have revolutionized natural language processing research and demonstrated potential in Artificial General Intelligence (AGI). However, the expensive training and deployment of LLMs present challenges to transparent and open academic research. To address these issues, this project open-sources the Chinese LLaMA and Alpaca large models, emphasizing instruction fine-tuning. We expand the original LLaMA\\'s Chinese vocabulary by adding 20K Chinese tokens, increasing encoding efficiency and enhancing basic semantic understanding. By incorporating secondary pre-training using Chinese data and fine-tuning with Chinese instruction data, we substantially improve the models\\' comprehension and execution of instructions. Our pilot study serves as a foundation for researchers adapting LLaMA and Alpaca models to other languages. Resources are made publicly available through GitHub, fostering open research in the Chinese NLP community and beyond. GitHub repository:\\nTags: [\\'ChatGPT\\', \\'GPT-4\\', \\'LLaMA\\', \\'Alpaca\\']\\n\\nAbstract: We propose VisFusion, a visibility-aware online 3D scene reconstruction approach from posed monocular videos. In particular, we aim to reconstruct the scene from volumetric features. Unlike previous reconstruction methods which aggregate features for each voxel from input views without considering its visibility, we aim to improve the feature fusion by explicitly inferring its visibility from a similarity matrix, computed from its projected features in each image pair. Following previous works, our model is a coarse-to-fine pipeline including a volume sparsification process. Different from their works which sparsify voxels globally with a fixed occupancy threshold, we perform the sparsification on a local feature volume along each visual ray to preserve at least one voxel per ray for more fine details. The sparse local volume is then fused with a global one for online reconstruction. We further propose to predict TSDF in a coarse-to-fine manner by learning its residuals across scales leading to better TSDF predictions. Experimental results on benchmarks show that our method can achieve superior performance with more scene details. Code is available at:\\nTags: [\\'NA\\']\\n\\n\\nAbstract: {input}\\nTags:'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "few_shot_template"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jvvfx8ae-hmf"
      },
      "source": [
        "### Zero-Shot Template\n",
        "\n",
        "The code below defines the zero-shot template. Note that we use the same instruction from the few-shot prompt template. But in this case, we don't use the demonstrations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "wYvzzwYA-hmf"
      },
      "outputs": [],
      "source": [
        "zero_shot_template = \"\"\"\n",
        "Your task is extract model names from machine learning paper abstracts. Your response is an an array of the model names in the format [\\\"model_name\\\"]. If you don't find model names in the abstract or you are not sure, return [\\\"NA\\\"]\n",
        "\n",
        "Abstract: {input}\n",
        "Tags:\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwvsFEof-hmf"
      },
      "source": [
        "### Get Predictions\n",
        "\n",
        "We then generated all the predictions using the validation data as inputs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XGhppRv--hmg"
      },
      "outputs": [],
      "source": [
        "# get the predictions\n",
        "\n",
        "abstracts = [val_data[i][\"abstract\"] for i in range(len(val_data))]\n",
        "few_shot_predictions = get_predictions(few_shot_template, abstracts)\n",
        "zero_shot_predictions = get_predictions(zero_shot_template, abstracts)\n",
        "expected_tags = [str(val_data[i][\"tags\"]) for i in range(len(val_data))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5to6iSU2-hmg",
        "outputId": "bcb9e84a-8215-4636-e85a-dad2b305a640"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Few shot predictions\n",
            "[\"['LLM', 'ChatGPT', 'LLaMA', 'WizardLM', 'OpenAI ChatGPT']\", \"['FLAN-T5', 'AMR', 'UD', 'SRL', 'LoRA']\", '[\"NA\"]', \"['ChatGPT', 'GPT-4', 'LLaMA', 'Alpaca', 'GMMSeg', 'GMMs', 'PAXQA']\", \"['ChatGPT']\", \"['ViT', 'OpenCLIP']\", \"['SAM', 'IA', 'AIGC', 'Stable Diffusion']\", \"['Anything-3D', 'BLIP', 'Segment-Anything']\", \"['Chameleon', 'LLMs', 'GPT-4', 'ScienceQA', 'TabMWP', 'ChatGPT']\", \"['NA']\"]\n",
            "\n",
            "\n",
            "Zero shot predictions\n",
            "['[\"WizardLM\", \"Evol-Instruct\", \"LLaMA\", \"ChatGPT\"]', '[\"FLAN-T5\"]', '[\"large language models\", \"generative AI\", \"hypothesis machines\"]', '[\"PAXQA\"]', '[\"ChatGPT\"]', '[\"ViT\", \"OpenCLIP\"]', '[\"Segment-Anything Model (SAM)\", \"Inpaint Anything (IA)\", \"AIGC models\", \"Stable Diffusion\", \"Inpaint Anything (IA)\"]', '[\"Anything-3D\", \"BLIP\", \"Segment-Anything\", \"text-to-image diffusion model\"]', '[\"Chameleon\", \"GPT-4\", \"ScienceQA\", \"TabMWP\", \"ChatGPT\"]', '[\"NA\"]']\n",
            "\n",
            "\n",
            "Expected tags\n",
            "[\"['LLaMA', 'ChatGPT', 'WizardLM']\", \"['FLAN-T5', 'FLAN']\", \"['NA']\", \"['PAXQA']\", \"['ChatGPT']\", \"['OpenCLIP', 'ViT']\", \"['SAM', 'IA']\", \"['Anything-3D', 'BLIP', 'Segment-Anything']\", \"['Chameleon', 'GPT-4', 'ChatGPT']\", \"['NA']\"]\n"
          ]
        }
      ],
      "source": [
        "print(\"Few shot predictions\")\n",
        "print(few_shot_predictions)\n",
        "print(\"\\n\\nZero shot predictions\")\n",
        "print(zero_shot_predictions)\n",
        "print(\"\\n\\nExpected tags\")\n",
        "print(expected_tags)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vq47EV18-hmg"
      },
      "source": [
        "### Log Prompt Results\n",
        "\n",
        "Finally, we log the prompt + results to Comet. Note that we are logging both the few-shot and zero-shot results, together with all the metadata and tags."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iBhWMBZa-hmg",
        "outputId": "c982ef14-e3e7-498f-9fb7-cfab6581fcf5"
      },
      "outputs": [],
      "source": [
        "# log the predictions in Comet along with the ground truth for comparison\n",
        "\n",
        "# set up comet\n",
        "# COMET_API_KEY = \"COMET_API_KEY\"\n",
        "# COMET_WORKSPACE = \"COMET_WORKSPACE\"\n",
        "\n",
        "# initialize comet\n",
        "# experiment = comet_ml.start(api_key=COMET_API_KEY, workspace=COMET_WORKSPACE, project_name=\"ml-paper-tagger-prompts\")\n",
        "\n",
        "# Sample test\n",
        "# few_shot_predictions = [\"['LLM', 'ChatGPT', 'LLaMA', 'WizardLM', 'OpenAI ChatGPT']\", \"['FLAN-T5', 'AMR', 'UD', 'SRL', 'LoRA']\", '[\"NA\"]', \"['ChatGPT', 'GPT-4', 'LLaMA', 'Alpaca', 'GMMSeg', 'GMMs', 'PAXQA']\", \"['ChatGPT']\", \"['ViT', 'OpenCLIP']\", \"['SAM', 'IA', 'AIGC', 'Stable Diffusion']\", \"['Anything-3D', 'BLIP', 'Segment-Anything']\", \"['Chameleon', 'LLMs', 'GPT-4', 'ScienceQA', 'TabMWP', 'ChatGPT']\", \"['NA']\"]\n",
        "# zero_shot_predictions = ['[\"WizardLM\", \"Evol-Instruct\", \"LLaMA\", \"ChatGPT\"]', '[\"FLAN-T5\"]', '[\"large language models\", \"generative AI\", \"hypothesis machines\"]', '[\"PAXQA\"]', '[\"ChatGPT\"]', '[\"ViT\", \"OpenCLIP\"]', '[\"Segment-Anything Model (SAM)\", \"Inpaint Anything (IA)\", \"AIGC models\", \"Stable Diffusion\", \"Inpaint Anything (IA)\"]', '[\"Anything-3D\", \"BLIP\", \"Segment-Anything\", \"text-to-image diffusion model\"]', '[\"Chameleon\", \"GPT-4\", \"ScienceQA\", \"TabMWP\", \"ChatGPT\"]', '[\"NA\"]']\n",
        "\n",
        "\n",
        "# log the predictions\n",
        "for i in range(len(expected_tags)):\n",
        "    # log the few-shot predictions\n",
        "    opik.Prompt(\n",
        "        name='few shot template', \n",
        "        prompt=few_shot_template.format(input=abstracts[i]),\n",
        "        metadata = {\n",
        "            \"expected_tags\": expected_tags[i],\n",
        "            \"abstract\": abstracts[i],\n",
        "        }\n",
        "        )\n",
        "    # comet_ml.log_prompt(\n",
        "    #     prompt=few_shot_template.format(input=abstracts[i]),\n",
        "    #     prompt_template=few_shot_template,\n",
        "    #     output=few_shot_predictions[i],\n",
        "    #     tags = [\"gpt-3.5-turbo\", \"few-shot\"],\n",
        "        # metadata = {\n",
        "        #     \"expected_tags\": expected_tags[i],\n",
        "        #     \"abstract\": abstracts[i],\n",
        "        # }\n",
        "    # )\n",
        "\n",
        "    # log the zero-shot predictions\n",
        "    opik.Prompt(\n",
        "        name='zero shot template', \n",
        "        prompt=zero_shot_template.format(input=abstracts[i]),\n",
        "        metadata = {\n",
        "            \"expected_tags\": expected_tags[i],\n",
        "            \"abstract\": abstracts[i],\n",
        "        }\n",
        "        )\n",
        "    # comet_ml.log_prompt(\n",
        "    #     prompt=zero_shot_template.format(input=abstracts[i]),\n",
        "    #     prompt_template=zero_shot_template,\n",
        "    #     output=zero_shot_predictions[i],\n",
        "    #     tags = [\"gpt-3.5-turbo\", \"zero-shot\"],\n",
        "    #     metadata = {\n",
        "    #         \"expected_tags\": expected_tags[i],\n",
        "    #         \"abstract\": abstracts[i],\n",
        "    #     }\n",
        "    # )"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
